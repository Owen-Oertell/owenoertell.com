<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <link rel="icon" type="image/svg+xml" sizes="any" href="/favicon.ico" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
		<link href="/_app/immutable/assets/0.BL-tmMuE.css" rel="stylesheet">
		<link href="/_app/immutable/assets/3.IoD07seF.css" rel="stylesheet">
		<link rel="modulepreload" href="/_app/immutable/entry/start.Dr3mmzAa.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/4oz8gmgq.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BBPmyNM4.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/DwrScgbY.js">
		<link rel="modulepreload" href="/_app/immutable/entry/app.CL_y1PA5.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/ClOl9dmH.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BZXONuDF.js">
		<link rel="modulepreload" href="/_app/immutable/nodes/0.BuVFTb2a.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CDJt_I-z.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/VOoZFfsa.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BSWeh_4s.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CChnDLSk.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CWSYvbSR.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/TSY2CcxY.js">
		<link rel="modulepreload" href="/_app/immutable/nodes/3.XKpBt9Ov.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BVP4zunF.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/zfra-goi.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/DnSXSBc1.js"><!--12qhfyh--><!--[--><script async src="https://www.googletagmanager.com/gtag/js?id=G-DTJQ3J1NVE"></script> <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-DTJQ3J1NVE");
    </script><!--]--><!----><!--gsrl61--><meta name="description" content="Dataset Reset Policy Optimization for RLHF"/> <meta property="og:title" content="Owen Oertell – DRPO"/> <meta property="og:description" content="Dataset Reset Policy Optimization for RLHF"/> <meta property="og:image" content="https://www.owenoertell.com/assets/images/image_round_1.png"/> <meta name="twitter:card" content="summary_large_image"/><!----><title>Owen Oertell – DRPO</title>
</head>

<body>
  <div><!--[--><!--[--><!----><header class="layout-md flex justify-between items-start" data-sveltekit-noscroll="" data-sveltekit-preload-code="eager"><h1 class="font-bold text-black text-2xl mb-6"><a href="/">Owen Oertell</a> <!--[!--><!--]--></h1> <nav class="svelte-1elxaub"><!--[--><a href="/#publications" class="hover:text-black transition-colors svelte-1elxaub">publications</a><a href="/resume" class="hover:text-black transition-colors svelte-1elxaub">resume</a><a href="/#contact" class="hover:text-black transition-colors svelte-1elxaub">contact</a><!--]--></nav></header><!----> <!--[!--><!----><main><!--[--><!----><!----> <section class="layout-md"><a href="/#publications" class="text-neutral-500 hover:text-neutral-700 text-sm mb-4 inline-block">← back</a> <h1 class="text-2xl font-bold text-black dark:text-white mb-2">Dataset Reset Policy Optimization for RLHF</h1> <p class="text-neutral-600 dark:text-neutral-400 mb-4">Jonathan Daniel Chang*, Wenhao Zhan*, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D. Lee, and Wen Sun
</p> <div class="flex flex-wrap gap-3 items-center mb-8"><span class="venue-tag svelte-13q5ovy">ArXiv</span> <!--[--><a href="https://arxiv.org/pdf/2404.08495" target="_blank" rel="noreferrer" class="action-btn svelte-13q5ovy">Paper <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-arrow-up-right"><!--[--><!----><path d="M7 7h10v10"><!----></path><!----><!----><path d="M7 17 17 7"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----></a><!--]--> <!--[--><button class="action-btn copy-btn svelte-13q5ovy"><span class="icon-wrapper svelte-13q5ovy"><!--[!--><span class="icon svelte-13q5ovy"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-copy"><!--[--><!----><rect width="14" height="14" x="8" y="8" rx="2" ry="2"><!----></rect><!----><!----><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----></span><!--]--></span> <span>Copy BibTeX</span></button><!--]--></div> <!--[--><div class="mb-8"><h2 class="text-sm uppercase text-neutral-500 mb-2">Abstract</h2> <div class="abstract-content text-neutral-700 dark:text-neutral-300 leading-relaxed svelte-13q5ovy"><!----><p>Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found <a href="https://github.com/Cornell-RL/drpo">here</a>.</p>
<!----></div></div><!--]--></section><!----><!--]--></main><!----><!--]--><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_oh8fdc = {
						base: ""
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("/_app/immutable/entry/start.Dr3mmzAa.js"),
						import("/_app/immutable/entry/app.CL_y1PA5.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data: [null,{type:"data",data:null,uses:{}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>