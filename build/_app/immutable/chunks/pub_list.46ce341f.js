var e={papers2025:[{title:"Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason",slug:"heuristics-considered-harmful",date:new Date(17553888e5),link:"https://fuchsia-arch-d8e.notion.site/Heuristics-Considered-Harmful-RL-With-Random-Rewards-Should-Not-Make-LLMs-Reason-21ba29497c4180ca86ffce303f01923d",authors:`Owen Oertell*, Wenhao Zhan*, Gokul Swamy, Zhiwei Steven Wu, Kiante Brantley, Jason Lee, Wen Sun
`,venue:"NYRL 2024",firstAuthor:!0,abstract:"Recent work has shown that for particular combinations of base model and training algorithm, reinforcement learning with random rewards (RLRR) improves the performance of LLMs on certain math reasoning benchmarks. This result is surprising as the (expected) policy gradient is exactly zero for RLRR, as all policies look the same under a random reward function. In response, we use RLRR as a diagnostic task for evaluating how well different classes of RL algorithms follow this true policy gradient. First, we demonstrate that algorithms that follow the (natural) policy gradient (e.g. RLoo (Kool et al., 2019) or REBEL (Gao et al., 2024)) produce the expected behavior of performance staying flat with random rewards, only increasing when provided with ground-truth rewards.  Second, we show that rather than holding steady, heuristic policy gradients like PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) can either increase or decrease the reasoning performance of the model considerably. Third, we demonstrate than on a didactic bandit problem — a problem that has nothing to do with LLMs or reasoning —  GRPO exhibits a bias towards choices that were more likely under the base policy, while the vanilla REINFORCE policy gradient (Williams, 1992) has no such tendencies. Taken together, our results underscore the importance of the choice of RL algorithm when making claims about LLM reasoning and beyond.",bibtex:`@misc{oertell2025heuristicsconsideredharmful,
  title={Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason}, 
  author={Owen Oertell and Wenhao Zhan and Gokul Swamy and Zhiwei Steven Wu and Kiante Brantley and Jason Lee and Wen Sun},
  year={2025},
  url={https://fuchsia-arch-d8e.notion.site/Heuristics-Considered-Harmful-RL-With-Random-Rewards-Should-Not-Make-LLMs-Reason-21ba29497c4180ca86ffce303f01923d},
}
`,shortName:"Heuristics Considered Harmful"},{title:"Efficient Controllable Diffusion via Optimal Classifier Guidance",slug:"slcd",date:new Date(1748304e6),link:"https://arxiv.org/pdf/2505.21666",authors:`Owen Oertell*, Shikun Sun*, Yiding Chen*, Jin Peng Zhou, Zhiyong Wang, and Wen Sun
`,venue:"ArXiv",firstAuthor:!0,abstract:"The controllable generation of diffusion models aims to steer the model to generate samples that optimize some given objective functions. It is desirable for a variety of applications including image generation, molecule generation, and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of the base model is a popular approach but it can overfit the reward function while requiring significant resources. We frame controllable generation as a problem of finding a distribution that optimizes a KL-regularized objective function. We present SLCD -- Supervised Learning based Controllable Diffusion, which iteratively generates online data and trains a small classifier to guide the generation of the diffusion model. Similar to the standard classifier-guided diffusion, SLCD's key computation primitive is classification and does not involve any complex concepts from RL or control. Via a reduction to no-regret online learning analysis, we show that under KL divergence, the output from SLCD provably converges to the optimal solution of the KL-regularized objective. Further, we empirically demonstrate that SLCD can generate high quality samples with nearly the same inference time as the base model in both image generation with continuous diffusion and biological sequence generation with discrete diffusion. Our code is available [here](https://github.com/Owen-Oertell/slcd).",bibtex:`@misc{oertell2025efficientcontrollablediffusionoptimal,
  title={Efficient Controllable Diffusion via Optimal Classifier Guidance}, 
  author={Owen Oertell and Shikun Sun and Yiding Chen and Jin Peng Zhou and Zhiyong Wang and Wen Sun},
  year={2025},
  eprint={2505.21666},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2505.21666}, 
}
`,shortName:"SLCD"},{title:"Improved Bounds for Delay Dependent Bandits",slug:"delay-dependent-bandits",date:new Date(1748304e6),authors:`Owen Oertell*, Ahan Mishra*, Parker Rho, and Robert Kleinberg
`,venue:"Preprint",firstAuthor:!0,abstract:"Delay dependent bandits are a popular class of bandit problems where the each arm's payoff function is dependent on the time since it was last pulled. These problems are popular for representing time dependence in choices. In this work, we consider two similar scenarios, the recharging bandit setting and the monotone last switch dependent setting, and provide tighter bounds for each of these through new algorithms and analysis. In particular, we show a new high probability bound for reward in the last switch dependent setting and a 5/6 approximation bound for the recharging bandit setting. Our analysis relies on no further assumptions for each of the settings. ",shortName:"Delay-Dependent Bandits"},{title:"Scaling Offline RL via Efficient and Expressive Shortcut Models",slug:"sorl",date:new Date(17483904e5),link:"https://arxiv.org/pdf/2505.22866",authors:`Nicolas Espinosa-Dice, Yiyi Zhang, Yiding Chen, Bradley Guo, Owen Oertell, Gokul Swamy, Kiante Brantley, and Wen Sun
`,venue:"NeurIPS 2025",abstract:"Diffusion and flow models have emerged as powerful generative approaches capable of modeling diverse and multimodal behavior. However, applying these models to offline reinforcement learning (RL) remains challenging due to the iterative nature of their noise sampling processes, making policy optimization difficult. In this paper, we introduce Scalable Offline Reinforcement Learning (`SORL`), a new offline RL algorithm that leverages shortcut models--a novel class of generative models--to scale both training and inference. `SORL`'s policy can capture complex data distributions and can be trained simply and efficiently in a one-stage training procedure. At test time, `SORL` introduces both sequential and parallel inference scaling by using the learned Q-function as a verifier. We demonstrate that `SORL` achieves strong performance across a range of offline RL tasks and exhibits positive scaling behavior with increased test-time compute. We release the code [here](https://nico-espinosadice.github.io/projects/sorl).",bibtex:`@article{espinosa2025scaling,
  title={Scaling Offline RL via Efficient and Expressive Shortcut Models},
  author={Nicolas Espinosa-Dice and Yiyi Zhang and Yiding Chen and Bradley Guo and Owen Oertell and Gokul Swamy and Kiante Brantley and Wen Sun},
  journal={Neural Information Processing Symposium (NeurIPS)},
  year={2025}
}
`,shortName:"SORL"},{title:"Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions",slug:"consistency-model-convergence",date:new Date(17464896e5),link:"https://arxiv.org/pdf/2505.03194",authors:`Yiding Chen, Yiyi Zhang, Owen Oertell, Wen Sun
`,venue:"ICML 2025",abstract:"Diffusion models accomplish remarkable success in data generation tasks across various domains. However, the iterative sampling process is computationally expensive. Consistency models are proposed to learn consistency functions to map from noise to data directly, which allows one-step fast data generation and multistep sampling to improve sample quality. In this paper, we study the convergence of consistency models when the self-consistency property holds approximately under the training distribution. Our analysis requires only mild data assumption and applies to a family of forward processes. When the target data distribution has bounded support or has tails that decay sufficiently fast, we show that the samples generated by the consistency model are close to the target distribution in Wasserstein distance; when the target distribution satisfies some smoothness assumption, we show that with an additional perturbation step for smoothing, the generated samples are close to the target distribution in total variation distance. We provide two case studies with commonly chosen forward processes to demonstrate the benefit of multistep sampling.",bibtex:`@inproceedings{
  chen2025convergence,
  title={Convergence of Consistency Model with Multistep Sampling under General Data Assumptions},
  author={Yiding Chen and Yiyi Zhang and Owen Oertell and Wen Sun},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025},
  url={https://openreview.net/forum?id=vsJsR3ieCx}
}
`,shortName:"Consistency Model Convergence"}],papers2024:[{title:"TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models",slug:"turbohopp",date:new Date(17300736e5),link:"https://arxiv.org/pdf/2410.20660",authors:`Kiwoong Yoo, Owen Oertell, Junhyun Lee, Sanghoon Lee, Jaewoo Kang
`,venue:"NeurIPS 2024",abstract:"Navigating the vast chemical space of druggable compounds is a formidable challenge in drug discovery, where generative models are increasingly employed to identify viable candidates. Conditional 3D structure-based drug design (3D-SBDD) models, which take into account complex three-dimensional interactions and molecular geometries, are particularly promising. Scaffold hopping is an efficient strategy that facilitates the identification of similar active compounds by strategically modifying the core structure of molecules, effectively narrowing the wide chemical space and enhancing the discovery of drug-like products. However, the practical application of 3D-SBDD generative models is hampered by their slow processing speeds. To address this bottleneck, we introduce TurboHopp, an accelerated pocket-conditioned 3D scaffold hopping model that merges the strategic effectiveness of traditional scaffold hopping with rapid generation capabilities of consistency models. This synergy not only enhances efficiency but also significantly boosts generation speeds, achieving up to 30 times faster inference speed as well as superior generation quality compared to existing diffusion-based models, establishing TurboHopp as a powerful tool in drug discovery. Supported by faster inference speed, we further optimize our model, using [Reinforcement Learning for Consistency Models (RLCM)](/publications/rlcm), to output desirable molecules. We demonstrate the broad applicability of TurboHopp across multiple drug discovery scenarios, underscoring its potential in diverse molecular settings. The code is provided [here](https://github.com/orgw/TurboHopp).",bibtex:`@inproceedings{
  yoo2024turbohopp,
  title={TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models},
  author={Kiwoong Yoo and Owen Oertell and Junhyun Lee and Sanghoon Lee and Jaewoo Kang},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=lBh5kuuY1L}
}
`,shortName:"TurboHopp"},{title:"REBEL: Reinforcement Learning via Regressing Relative Rewards",slug:"rebel",date:new Date(17140032e5),link:"https://arxiv.org/pdf/2404.16767",authors:`Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun
`,venue:"NeurIPS 2024",abstract:"While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative reward between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.",bibtex:`@inproceedings{
  gao2024rebel,
  title={{REBEL}: Reinforcement Learning via Regressing Relative Rewards},
  author={Zhaolin Gao and Jonathan Daniel Chang and Wenhao Zhan and Owen Oertell and Gokul Swamy and Kiant{\\'e} Brantley and Thorsten Joachims and J. Andrew Bagnell and Jason D. Lee and Wen Sun},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=yxjWAJzUyV}
}
`,shortName:"REBEL"},{title:"RL for Consistency Models: Reward Guided Text-to-Image Generation with Fast Inference",slug:"rlcm",date:new Date(1709856e6),link:"https://arxiv.org/pdf/2404.03673",authors:`Owen Oertell, Jonathan Daniel Chang, Yiyi Zhang, Kianté Brantley, and Wen Sun
`,venue:"RLC 2024",firstAuthor:!0,abstract:"Reinforcement learning (RL) has improved guided image generation with diffusion models by directly optimizing rewards that capture image quality, aesthetics, and instruction following capabilities. However, the resulting generative policies inherit the same iterative sampling process of diffusion models that causes slow generation. To overcome this limitation, consistency models proposed learning a new class of generative models that directly map noise to data, resulting in a model that can generate an image in as few as one sampling iteration. In this work, to optimize text-to-image generative models for task specific rewards and enable fast training and inference, we propose a framework for fine-tuning consistency models via RL. Our framework, called Reinforcement Learning for Consistency Model (RLCM), frames the iterative inference process of a consistency model as an RL procedure. Comparing to RL finetuned diffusion models, RLCM trains significantly faster, improves the quality of the generation measured under the reward objectives, and speeds up the inference procedure by generating high quality images with as few as two inference steps. Experimentally, we show that RLCM can adapt text-to-image consistency models to objectives that are challenging to express with prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Our code is available [here](https://rlcm.owenoertell.com).",bibtex:`@article{oertell2024rlcm,
  author={Owen Oertell and Jonathan D. Chang and Yiyi Zhang and Kianté Brantley and Wen Sun},
  title={RL for Consistency Models: Reward Guided Text-to-Image Generation with Fast Inference},
  year={2024},
  cdate={1704067200000},
  journal={RLJ},
  volume={4},
  pages={1656-1673},
  url={https://rlj.cs.umass.edu/2024/papers/Paper210.html}
}
`,shortName:"RLCM"},{title:"More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning",slug:"distrl",date:new Date(17067456e5),authors:`Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, and Wen Sun
`,venue:"ICML 2024",link:"https://arxiv.org/pdf/2402.07198",abstract:"In this paper, we prove that Distributional Reinforcement Learning (DistRL), which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation. Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of distributional RL. To the best of our knowledge, our results are the first second-order bounds for low-rank MDPs and for offline RL. When specializing to contextual bandits (one-step RL problem), we show that a distributional learning based optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously. We also empirically demonstrate the benefit of DistRL in contextual bandits on real-world datasets. We highlight that our analysis with DistRL is relatively simple, follows the general framework of optimism in the face of uncertainty and does not require weighted regression. Our results suggest that DistRL is a promising framework for obtaining second-order bounds in general RL settings, thus further reinforcing the benefits of DistRL.",bibtex:`@inproceedings{
  wang2024more,
  title={More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning},
  author={Kaiwen Wang and Owen Oertell and Alekh Agarwal and Nathan Kallus and Wen Sun},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=kZBCFQe1Ej}
}
`,shortName:"DistRL"},{title:"Dataset Reset Policy Optimization for RLHF",slug:"drpo",date:new Date(17067456e5),link:"https://arxiv.org/pdf/2404.08495",authors:`Jonathan Daniel Chang*, Wenhao Zhan*, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D. Lee, and Wen Sun
`,venue:"ArXiv",abstract:"Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found [here](https://github.com/Cornell-RL/drpo).",bibtex:`@misc{chang2024datasetresetpolicyoptimization,
  title={Dataset Reset Policy Optimization for RLHF}, 
  author={Jonathan D. Chang and Wenhao Zhan and Owen Oertell and Kianté Brantley and Dipendra Misra and Jason D. Lee and Wen Sun},
  year={2024},
  eprint={2404.08495},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2404.08495}, 
}
`,shortName:"DRPO"}],papers2023:[{title:"A Kernel Method Approach to Orbital Debris Blast Point Determination",slug:"kernel-orbital-debris",date:new Date(17022528e5),link:"https://arc.aiaa.org/doi/10.2514/6.2024-1864",authors:`Jackson Kulik, Owen Oertell, and Dmitry Savransky
`,venue:"AIAA 2024",abstract:"Given a known parent object, we determine the epoch and velocity distribution of an orbital break-up given unassociated observations of the debris cloud positions at multiple times. Using a tentative guess of the break-up time, we solve Lambert's problem to determine an initial velocity distribution for each debris cloud observation. Agreement of proposed initial velocity distributions depends on the assumed break-up time. We use maximal mean discrepancy in a reproducing kernel Hilbert space to quantify the difference between tentative initial velocity distributions, and optimize this metric to determine the time of break-up.",bibtex:`@inbook{kulik2024kernel,
  author = {Jackson Kulik and Owen Oertell and Dmitry Savransky},
  title={A Kernel Method Approach to Orbital Debris Blast Point Determination},
  booktitle={AIAA SCITECH 2024 Forum},
  year={2024},
  doi={10.2514/6.2024-1864},
  url={https://arc.aiaa.org/doi/abs/10.2514/6.2024-1864}
}
`,shortName:"Kernel Orbital Debris"},{title:"Overdetermined Eigenvector Approach to Passive Angles-Only Relative Orbit Determination",slug:"eigenvector-orbit-determination",date:new Date(16847136e5),link:"https://arc.aiaa.org/doi/10.2514/1.G007744",authors:`Jackson Kulik, Owen Oertell, and Dmitry Savransky
`,venue:"JGCD 2023",abstract:"",bibtex:`@article{kulik2024overdetermined,
  author = {Jackson Kulik and Owen Oertell and Dmitry Savransky},
  title={Overdetermined Eigenvector Approach to Passive Angles-Only Relative Orbit Determination},
  journal={Journal of Guidance, Control, and Dynamics},
  volume={47},
  number={5},
  pages={986-994},
  year={2024},
  doi={10.2514/1.G007744},
  url={https://doi.org/10.2514/1.G007744}
}
`,shortName:"Eigenvector Orbit Determination"}]};export{e as d};
