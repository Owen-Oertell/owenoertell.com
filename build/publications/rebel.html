<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <link rel="icon" type="image/svg+xml" sizes="any" href="/favicon.ico" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
		<link href="/_app/immutable/assets/0.BL-tmMuE.css" rel="stylesheet">
		<link href="/_app/immutable/assets/3.IoD07seF.css" rel="stylesheet">
		<link rel="modulepreload" href="/_app/immutable/entry/start.Dr3mmzAa.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/4oz8gmgq.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BBPmyNM4.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/DwrScgbY.js">
		<link rel="modulepreload" href="/_app/immutable/entry/app.CL_y1PA5.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/ClOl9dmH.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BZXONuDF.js">
		<link rel="modulepreload" href="/_app/immutable/nodes/0.BuVFTb2a.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CDJt_I-z.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/VOoZFfsa.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BSWeh_4s.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CChnDLSk.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CWSYvbSR.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/TSY2CcxY.js">
		<link rel="modulepreload" href="/_app/immutable/nodes/3.XKpBt9Ov.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BVP4zunF.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/zfra-goi.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/DnSXSBc1.js"><!--12qhfyh--><!--[--><script async src="https://www.googletagmanager.com/gtag/js?id=G-DTJQ3J1NVE"></script> <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-DTJQ3J1NVE");
    </script><!--]--><!----><!--gsrl61--><meta name="description" content="REBEL: Reinforcement Learning via Regressing Relative Rewards"/> <meta property="og:title" content="Owen Oertell – REBEL"/> <meta property="og:description" content="REBEL: Reinforcement Learning via Regressing Relative Rewards"/> <meta property="og:image" content="https://www.owenoertell.com/assets/images/image_round_1.png"/> <meta name="twitter:card" content="summary_large_image"/><!----><title>Owen Oertell – REBEL</title>
</head>

<body>
  <div><!--[--><!--[--><!----><header class="layout-md flex justify-between items-start" data-sveltekit-noscroll="" data-sveltekit-preload-code="eager"><h1 class="font-bold text-black text-2xl mb-6"><a href="/">Owen Oertell</a> <!--[!--><!--]--></h1> <nav class="svelte-1elxaub"><!--[--><a href="/#publications" class="hover:text-black transition-colors svelte-1elxaub">publications</a><a href="/resume" class="hover:text-black transition-colors svelte-1elxaub">resume</a><a href="/#contact" class="hover:text-black transition-colors svelte-1elxaub">contact</a><!--]--></nav></header><!----> <!--[!--><!----><main><!--[--><!----><!----> <section class="layout-md"><a href="/#publications" class="text-neutral-500 hover:text-neutral-700 text-sm mb-4 inline-block">← back</a> <h1 class="text-2xl font-bold text-black dark:text-white mb-2">REBEL: Reinforcement Learning via Regressing Relative Rewards</h1> <p class="text-neutral-600 dark:text-neutral-400 mb-4">Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun
</p> <div class="flex flex-wrap gap-3 items-center mb-8"><span class="venue-tag svelte-13q5ovy">NeurIPS 2024</span> <!--[--><a href="https://arxiv.org/pdf/2404.16767" target="_blank" rel="noreferrer" class="action-btn svelte-13q5ovy">Paper <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-arrow-up-right"><!--[--><!----><path d="M7 7h10v10"><!----></path><!----><!----><path d="M7 17 17 7"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----></a><!--]--> <!--[--><button class="action-btn copy-btn svelte-13q5ovy"><span class="icon-wrapper svelte-13q5ovy"><!--[!--><span class="icon svelte-13q5ovy"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-copy"><!--[--><!----><rect width="14" height="14" x="8" y="8" rx="2" ry="2"><!----></rect><!----><!----><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----></span><!--]--></span> <span>Copy BibTeX</span></button><!--]--></div> <!--[--><div class="mb-8"><h2 class="text-sm uppercase text-neutral-500 mb-2">Abstract</h2> <div class="abstract-content text-neutral-700 dark:text-neutral-300 leading-relaxed svelte-13q5ovy"><!----><p>While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative reward between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.</p>
<!----></div></div><!--]--></section><!----><!--]--></main><!----><!--]--><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_oh8fdc = {
						base: ""
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("/_app/immutable/entry/start.Dr3mmzAa.js"),
						import("/_app/immutable/entry/app.CL_y1PA5.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data: [null,{type:"data",data:null,uses:{}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>