<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <link rel="icon" type="image/svg+xml" sizes="any" href="/favicon.ico" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
		<link href="/_app/immutable/assets/0.BL-tmMuE.css" rel="stylesheet">
		<link href="/_app/immutable/assets/3.IoD07seF.css" rel="stylesheet">
		<link rel="modulepreload" href="/_app/immutable/entry/start.Dr3mmzAa.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/4oz8gmgq.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BBPmyNM4.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/DwrScgbY.js">
		<link rel="modulepreload" href="/_app/immutable/entry/app.CL_y1PA5.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/ClOl9dmH.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BZXONuDF.js">
		<link rel="modulepreload" href="/_app/immutable/nodes/0.BuVFTb2a.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CDJt_I-z.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/VOoZFfsa.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BSWeh_4s.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CChnDLSk.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/CWSYvbSR.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/TSY2CcxY.js">
		<link rel="modulepreload" href="/_app/immutable/nodes/3.XKpBt9Ov.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/BVP4zunF.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/zfra-goi.js">
		<link rel="modulepreload" href="/_app/immutable/chunks/DnSXSBc1.js"><!--12qhfyh--><!--[--><script async src="https://www.googletagmanager.com/gtag/js?id=G-DTJQ3J1NVE"></script> <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-DTJQ3J1NVE");
    </script><!--]--><!----><!--gsrl61--><meta name="description" content="Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason"/> <meta property="og:title" content="Owen Oertell – Heuristics Considered Harmful"/> <meta property="og:description" content="Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason"/> <meta property="og:image" content="https://www.owenoertell.com/assets/images/image_round_1.png"/> <meta name="twitter:card" content="summary_large_image"/><!----><title>Owen Oertell – Heuristics Considered Harmful</title>
</head>

<body>
  <div><!--[--><!--[--><!----><header class="layout-md flex justify-between items-start" data-sveltekit-noscroll="" data-sveltekit-preload-code="eager"><h1 class="font-bold text-black text-2xl mb-6"><a href="/">Owen Oertell</a> <!--[!--><!--]--></h1> <nav class="svelte-1elxaub"><!--[--><a href="/#publications" class="hover:text-black transition-colors svelte-1elxaub">publications</a><a href="/resume" class="hover:text-black transition-colors svelte-1elxaub">resume</a><a href="/#contact" class="hover:text-black transition-colors svelte-1elxaub">contact</a><!--]--></nav></header><!----> <!--[!--><!----><main><!--[--><!----><!----> <section class="layout-md"><a href="/#publications" class="text-neutral-500 hover:text-neutral-700 text-sm mb-4 inline-block">← back</a> <h1 class="text-2xl font-bold text-black dark:text-white mb-2">Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason</h1> <p class="text-neutral-600 dark:text-neutral-400 mb-4">Owen Oertell*, Wenhao Zhan*, Gokul Swamy, Zhiwei Steven Wu, Kiante Brantley, Jason Lee, Wen Sun
</p> <div class="flex flex-wrap gap-3 items-center mb-8"><span class="venue-tag svelte-13q5ovy">NYRL 2024</span> <!--[--><a href="https://fuchsia-arch-d8e.notion.site/Heuristics-Considered-Harmful-RL-With-Random-Rewards-Should-Not-Make-LLMs-Reason-21ba29497c4180ca86ffce303f01923d" target="_blank" rel="noreferrer" class="action-btn svelte-13q5ovy">Paper <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-arrow-up-right"><!--[--><!----><path d="M7 7h10v10"><!----></path><!----><!----><path d="M7 17 17 7"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----></a><!--]--> <!--[--><button class="action-btn copy-btn svelte-13q5ovy"><span class="icon-wrapper svelte-13q5ovy"><!--[!--><span class="icon svelte-13q5ovy"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-icon lucide lucide-copy"><!--[--><!----><rect width="14" height="14" x="8" y="8" rx="2" ry="2"><!----></rect><!----><!----><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"><!----></path><!----><!--]--><!--[--><!--[--><!--]--><!--]--></svg><!----></span><!--]--></span> <span>Copy BibTeX</span></button><!--]--></div> <!--[--><div class="mb-8"><h2 class="text-sm uppercase text-neutral-500 mb-2">Abstract</h2> <div class="abstract-content text-neutral-700 dark:text-neutral-300 leading-relaxed svelte-13q5ovy"><!----><p>Recent work has shown that for particular combinations of base model and training algorithm, <em>reinforcement learning with random rewards</em> (RLRR) improves the performance of LLMs on certain math reasoning benchmarks. This result is surprising as the (expected) policy gradient is <em>exactly</em> zero for RLRR, as all policies look the same under a random reward function. In response, we use RLRR as a <em>diagnostic task</em> for evaluating how well different classes of RL algorithms follow this true policy gradient. First, we demonstrate that algorithms that follow the (natural) policy gradient (e.g. RLoo (Kool et al., 2019) or REBEL (Gao et al., 2024)) produce the expected behavior of performance staying flat with random rewards, only increasing when provided with ground-truth rewards.  Second, we show that rather than holding steady, <em>heuristic</em> policy gradients like PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) can either increase or decrease the reasoning performance of the model considerably. Third, we demonstrate than on a didactic bandit problem — a problem that has nothing to do with LLMs or reasoning —  GRPO exhibits a bias towards choices that were more likely under the base policy, while the vanilla REINFORCE policy gradient (Williams, 1992) has no such tendencies. Taken together, our results underscore the importance of the choice of RL algorithm when making claims about LLM reasoning and beyond.</p>
<!----></div></div><!--]--></section><!----><!--]--></main><!----><!--]--><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_oh8fdc = {
						base: ""
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("/_app/immutable/entry/start.Dr3mmzAa.js"),
						import("/_app/immutable/entry/app.CL_y1PA5.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data: [null,{type:"data",data:null,uses:{}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>

</html>